{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wav2vec2 experiment language model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dff991caac6740af82ca0f39aca7d3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8120b66af4c9469aae1a1d8a0c08551d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8a36111b22264b0f8a8bcfa45064753b",
              "IPY_MODEL_042fa86b18c34fb88410859b802311b0"
            ]
          }
        },
        "8120b66af4c9469aae1a1d8a0c08551d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a36111b22264b0f8a8bcfa45064753b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_488f55f91a764fc8a07c33ca276cda0d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 291,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 291,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ac9856aceb1499b9ef237eef1e47e63"
          }
        },
        "042fa86b18c34fb88410859b802311b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_447b6fa55b1a4a5f951b0d2a5f150343",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 291/291 [00:12&lt;00:00, 22.8B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ff3417ff35047b7a4db64175d40fc1a"
          }
        },
        "488f55f91a764fc8a07c33ca276cda0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ac9856aceb1499b9ef237eef1e47e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "447b6fa55b1a4a5f951b0d2a5f150343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ff3417ff35047b7a4db64175d40fc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a892829e8de4fe9809f957f8f247093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0604c47fe29f4fcba3f544545c5b310e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fbcf21da238343a696461b5db2d31e04",
              "IPY_MODEL_5bcf40f9910646269af409b03976abbf"
            ]
          }
        },
        "0604c47fe29f4fcba3f544545c5b310e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbcf21da238343a696461b5db2d31e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c4e60689278f4e009b011310323f90a5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 163,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 163,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2b487399d23405aad246401ec3bec81"
          }
        },
        "5bcf40f9910646269af409b03976abbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_17a70a4f1be64cfaa780dfa5a48aee83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 163/163 [00:00&lt;00:00, 794B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c107c394f0e4bdfb509b42b2bffae82"
          }
        },
        "c4e60689278f4e009b011310323f90a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2b487399d23405aad246401ec3bec81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17a70a4f1be64cfaa780dfa5a48aee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c107c394f0e4bdfb509b42b2bffae82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af3c44571f0b4682bd20c3ff303f31e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b26cd0fabde14cfdb159a543aeab1d7b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41d24e5f9de54d6f87afd30be06bb751",
              "IPY_MODEL_f88181f6ad214008875919ff21153e16"
            ]
          }
        },
        "b26cd0fabde14cfdb159a543aeab1d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41d24e5f9de54d6f87afd30be06bb751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76b8333bf211458db4fadbbc9ff08ccf",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 85,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 85,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0788778daae94e6fb96af7761e791517"
          }
        },
        "f88181f6ad214008875919ff21153e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7439e31eb9b04957aa52dcbd2c5f3182",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 85.0/85.0 [00:00&lt;00:00, 254B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0268a5f77baf4e8299f3bedd5ffb1e20"
          }
        },
        "76b8333bf211458db4fadbbc9ff08ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0788778daae94e6fb96af7761e791517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7439e31eb9b04957aa52dcbd2c5f3182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0268a5f77baf4e8299f3bedd5ffb1e20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a06b7d74d9564bfe97c0ffc53d53ea73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_22c74cd034404431bb6d3146aaadb813",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d3bcc1b01160411fa25fc7c7e1a2c296",
              "IPY_MODEL_02603ef2fd1c4901a3d7a9a06dbeeb4b"
            ]
          }
        },
        "22c74cd034404431bb6d3146aaadb813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3bcc1b01160411fa25fc7c7e1a2c296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a03d8dffb0f44041a1f28e9327309a8f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 843,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 843,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_033648514b5d481186f21765166f00cc"
          }
        },
        "02603ef2fd1c4901a3d7a9a06dbeeb4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_736e05ee095e4a329f6c7e69cf295fed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 843/843 [00:10&lt;00:00, 81.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a1ebfa1590b4c9ebf8e7e2c9cb26de9"
          }
        },
        "a03d8dffb0f44041a1f28e9327309a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "033648514b5d481186f21765166f00cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "736e05ee095e4a329f6c7e69cf295fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a1ebfa1590b4c9ebf8e7e2c9cb26de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f3bfdc1136844b88d49c04b5a14c092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_479414e9210046189fbb4ce9794cf87e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_63da5e5cc0b44b5c950e4fb063daa07f",
              "IPY_MODEL_44675892b72643c0a0123595e74881d6"
            ]
          }
        },
        "479414e9210046189fbb4ce9794cf87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63da5e5cc0b44b5c950e4fb063daa07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ae1153e3d1b46eb8f62b5ef211a548a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 377667514,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 377667514,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd16bc1442714a5aa546406746263d03"
          }
        },
        "44675892b72643c0a0123595e74881d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14c8253fd0294c33bb676f31b9713f7c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 378M/378M [00:08&lt;00:00, 42.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e271031504645bfacb171a0f95cffa4"
          }
        },
        "3ae1153e3d1b46eb8f62b5ef211a548a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd16bc1442714a5aa546406746263d03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14c8253fd0294c33bb676f31b9713f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e271031504645bfacb171a0f95cffa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAl-hAdOS8h"
      },
      "source": [
        "# Install ffmpeg-python\n",
        "for recorded audio decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEsMy-GvGIlu"
      },
      "source": [
        "!pip install -q ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C1Lv1DEOZJc"
      },
      "source": [
        "# Installing transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnNdtT2y7RL9",
        "outputId": "95e366ec-9cfb-422b-e892-fafb891188f2"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 56.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 55.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz6CiagtO-sO"
      },
      "source": [
        "# Installing ctcdecode\n",
        "Implementation of beam search algorithm for python. This is used to rescore output symbols from model using language model (kenlm in this case)\n",
        "\n",
        "kenlm is a n-gram language model small and efficient.\n",
        "We can use neural network based networks but it will be more resource intensive so kenlm is best choice to use as language model and is already used in many projects.\n",
        "\n",
        "By default ctcdecode use kenlm for rescoring so I do not bother to setup the model.\n",
        "- [ctcdecode repo](https://github.com/parlance/ctcdecode)\n",
        "- [kenlm](https://github.com/kpu/kenlm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PF4h9SgwHj",
        "outputId": "1d2bde62-1208-4b0a-91ac-adac719732c5"
      },
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!cd ctcdecode && pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1063, done.\u001b[K\n",
            "remote: Total 1063 (delta 0), reused 0 (delta 0), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1063/1063), 759.71 KiB | 9.38 MiB/s, done.\n",
            "Resolving deltas: 100% (513/513), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 13792, done.        \n",
            "remote: Counting objects: 100% (105/105), done.        \n",
            "remote: Compressing objects: 100% (58/58), done.        \n",
            "remote: Total 13792 (delta 59), reused 74 (delta 34), pack-reused 13687        \n",
            "Receiving objects: 100% (13792/13792), 5.48 MiB | 9.54 MiB/s, done.\n",
            "Resolving deltas: 100% (7939/7939), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Processing /content/ctcdecode\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.2-cp37-cp37m-linux_x86_64.whl size=12877952 sha256=3531e2739777dd8186c6f32ea1f92d255e1e9420348aee18e16d7763affab6d3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-05cjzmax/wheels/c3/6c/94/7d57d4f20a87a22ef1722eaad22052b4c435892b55400e5f4e\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QphzalcXOecZ"
      },
      "source": [
        "# Loading Dependencies\n",
        "- pytorch\n",
        "- Transformers library\n",
        "- numpy\n",
        "- ctcdecode (ctc beam search decoder with kenlm as language model)\n",
        "- librosa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WeEjJqMCFQD"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import ctcdecode\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1qpyMBxQqZi"
      },
      "source": [
        "# Instantiate pretrained models\n",
        "- Tokenizer\n",
        "- Wav2Vec2 Model\n",
        "\n",
        "The model takes as input a speech signal in any language (currently english because it was trained on english dataset) in its raw form. This audio data is one-dimensional and is passed to a multi-layer 1-d Convolutional neural network to generate audio representations of 25ms each"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "dff991caac6740af82ca0f39aca7d3e9",
            "8120b66af4c9469aae1a1d8a0c08551d",
            "8a36111b22264b0f8a8bcfa45064753b",
            "042fa86b18c34fb88410859b802311b0",
            "488f55f91a764fc8a07c33ca276cda0d",
            "7ac9856aceb1499b9ef237eef1e47e63",
            "447b6fa55b1a4a5f951b0d2a5f150343",
            "0ff3417ff35047b7a4db64175d40fc1a",
            "2a892829e8de4fe9809f957f8f247093",
            "0604c47fe29f4fcba3f544545c5b310e",
            "fbcf21da238343a696461b5db2d31e04",
            "5bcf40f9910646269af409b03976abbf",
            "c4e60689278f4e009b011310323f90a5",
            "a2b487399d23405aad246401ec3bec81",
            "17a70a4f1be64cfaa780dfa5a48aee83",
            "9c107c394f0e4bdfb509b42b2bffae82",
            "af3c44571f0b4682bd20c3ff303f31e1",
            "b26cd0fabde14cfdb159a543aeab1d7b",
            "41d24e5f9de54d6f87afd30be06bb751",
            "f88181f6ad214008875919ff21153e16",
            "76b8333bf211458db4fadbbc9ff08ccf",
            "0788778daae94e6fb96af7761e791517",
            "7439e31eb9b04957aa52dcbd2c5f3182",
            "0268a5f77baf4e8299f3bedd5ffb1e20",
            "a06b7d74d9564bfe97c0ffc53d53ea73",
            "22c74cd034404431bb6d3146aaadb813",
            "d3bcc1b01160411fa25fc7c7e1a2c296",
            "02603ef2fd1c4901a3d7a9a06dbeeb4b",
            "a03d8dffb0f44041a1f28e9327309a8f",
            "033648514b5d481186f21765166f00cc",
            "736e05ee095e4a329f6c7e69cf295fed",
            "9a1ebfa1590b4c9ebf8e7e2c9cb26de9",
            "7f3bfdc1136844b88d49c04b5a14c092",
            "479414e9210046189fbb4ce9794cf87e",
            "63da5e5cc0b44b5c950e4fb063daa07f",
            "44675892b72643c0a0123595e74881d6",
            "3ae1153e3d1b46eb8f62b5ef211a548a",
            "bd16bc1442714a5aa546406746263d03",
            "14c8253fd0294c33bb676f31b9713f7c",
            "5e271031504645bfacb171a0f95cffa4"
          ]
        },
        "id": "nJtBdPBBCNBa",
        "outputId": "4a92ca4c-813a-43db-ba54-02b09daf3b79"
      },
      "source": [
        "tokenizer = transformers.Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = transformers.Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dff991caac6740af82ca0f39aca7d3e9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=291.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a892829e8de4fe9809f957f8f247093",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=163.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af3c44571f0b4682bd20c3ff303f31e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=85.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:358: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06b7d74d9564bfe97c0ffc53d53ea73",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=843.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f3bfdc1136844b88d49c04b5a14c092",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=377667514.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_JD4gBWINib",
        "outputId": "bd63a982-e354-49dc-df71-6814e38c415a"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2ForCTC(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): Wav2Vec2FeatureExtractor(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): Wav2Vec2GroupNormConvLayer(\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "        )\n",
              "        (1): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (2): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (3): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (4): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (5): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        )\n",
              "        (6): Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_projection): Wav2Vec2FeatureProjection(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): Wav2Vec2Encoder(\n",
              "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
              "        (padding): Wav2Vec2SamePadLayer()\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layers): ModuleList(\n",
              "        (0): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2Attention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nneWZA34RJ-p"
      },
      "source": [
        "# Recording and loading audio in colab\n",
        "\n",
        "Taken from [ricardodeazambuja.com](https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBwBVbTMGZfy"
      },
      "source": [
        "# https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio(sr):\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  speech, rate = librosa.load(io.BytesIO(riff),sr=16000)\n",
        "  return speech, sr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "5eMRTtQoGaUs",
        "outputId": "77bd999b-007e-4f72-948b-fccde3feb054"
      },
      "source": [
        "#record or load any audio file of your choice here\n",
        "speech, rate = get_audio(sr=16000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Press to start recording\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Recording... press to stop\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPiVwIvcRfV2"
      },
      "source": [
        "# wave2vec2 vocabulary\n",
        "\n",
        "[found here](https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/vocab.json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_40iPXerI6AV"
      },
      "source": [
        "label_dict = {\"<pad>\": 0, \n",
        "              \"<s>\": 1, \n",
        "              \"</s>\": 2,\n",
        "              \"<unk>\": 3,\n",
        "              \"|\": 4,\n",
        "              \"E\": 5,\n",
        "              \"T\": 6,\n",
        "              \"A\": 7,\n",
        "              \"O\": 8,\n",
        "              \"N\": 9,\n",
        "              \"I\": 10,\n",
        "              \"H\": 11,\n",
        "              \"S\": 12,\n",
        "              \"R\": 13,\n",
        "              \"D\": 14,\n",
        "              \"L\": 15,\n",
        "              \"U\": 16,\n",
        "              \"M\": 17,\n",
        "              \"W\": 18,\n",
        "              \"C\": 19,\n",
        "              \"F\": 20,\n",
        "              \"G\": 21,\n",
        "              \"Y\": 22,\n",
        "              \"P\": 23,\n",
        "              \"B\": 24,\n",
        "              \"V\": 25,\n",
        "              \"K\": 26,\n",
        "              \"'\": 27,\n",
        "              \"X\": 28,\n",
        "              \"J\": 29,\n",
        "              \"Q\": 30,\n",
        "              \"Z\": 31\n",
        "              }\n",
        "labels = [key for key, value in label_dict.items()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889jjOC3SKlg"
      },
      "source": [
        "# CTC Beam search\n",
        "CTC Beam search algorithm combined with language model for rescoring probabilities output from language model.\n",
        "\n",
        "This class handles all the things we just need to pass our models softmax output into this class object to decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebojB-sFE6K"
      },
      "source": [
        "class CTCBeamDecoder:\n",
        "    def __init__(self, labels, blank_id=0, beam_size=100, kenlm_path=None):\n",
        "        print(\"loading beam search with kenlm...\")\n",
        "        self.labels = labels\n",
        "\n",
        "        # model_path = is the path to your external kenlm language model(LM). Default is none.\n",
        "        # alpha = Weighting associated with the LMs probabilities. A weight of 0 means the LM has no effect.\n",
        "        # beta = Weight associated with the number of words within our beam.\n",
        "\n",
        "        self.ctcdecoder = ctcdecode.CTCBeamDecoder(\n",
        "            self.labels, model_path=kenlm_path,\n",
        "            alpha=0.6, beta=1,\n",
        "            beam_width=beam_size, blank_id=blank_id)\n",
        "        print(\"loading finished\")\n",
        "\n",
        "    def __call__(self, output, num_sentences=1):\n",
        "        sentences = []\n",
        "        for num in range(num_sentences):\n",
        "          beam_result, beam_scores, timesteps, out_seq_len = self.ctcdecoder.decode(output)\n",
        "          # beam_result[0][0][:out_seq_len[0][0]] get the top beam for the first item in batch\n",
        "          sentences.append(self.output(beam_result[0][num], self.labels, out_seq_len[0][num]))\n",
        "        return sentences\n",
        "\n",
        "    def output(self, tokens, vocab, seq_len):\n",
        "        out = ''.join([vocab[x] for x in tokens[0:seq_len]])\n",
        "        # wave2vec implementation use | for space in vocabulary\n",
        "        return out.replace(\"|\", \" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI2zekfBjH53",
        "outputId": "ad2a347b-c39d-4ac2-a74e-6fb5a6bc82d4"
      },
      "source": [
        "# blank_id = ctc blank token (epsilon) which is <pad> in wave2vec vocabulary\n",
        "decode_and_rescore = CTCBeamDecoder(kenlm_path=None, \n",
        "                                          labels=labels, \n",
        "                                          blank_id=label_dict.get(\"<pad>\"), \n",
        "                                          beam_size=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading beam search with kenlm...\n",
            "loading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6BwpnfSVuTF"
      },
      "source": [
        "# Inferencing\n",
        "- tokenizing(encoding) speech data and return pytorch tensor\n",
        "- pass encodings to model\n",
        "- converting model outputs into probabilities using softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnqWGDbiFVd9"
      },
      "source": [
        "input_values = tokenizer(speech, return_tensors = 'pt').input_values\n",
        "#logits (non-normalized predictions)\n",
        "logits = model(input_values).logits\n",
        "out_proba = torch.nn.functional.softmax(logits, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izhvg-iSa6vB"
      },
      "source": [
        "predicted_ids = torch.argmax(out_proba, dim =-1)\n",
        "results_ = tokenizer.decode(predicted_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1NqltfmbMFK",
        "outputId": "d40d92e2-b1dd-4a3c-9619-7306107423d7"
      },
      "source": [
        "print(\"Without Language Model\")\n",
        "print(results_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Language Model\n",
            "THE BOOK IS ON THE TABLE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiLFjY33WL7m"
      },
      "source": [
        "# Applying rescoring algorithm \n",
        "using language model and beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEKolRm5dYoN"
      },
      "source": [
        "results = decode_and_rescore(out_proba, num_sentences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPoefQcKjmtL",
        "outputId": "a0121df1-b645-4fe2-8bab-448f062ca8eb"
      },
      "source": [
        "print(\"With Language Model Kenlm\")\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With Language Model Kenlm\n",
            "THE BOOK IS ON THE TABLE \n",
            "THE BOOK IS ON THER TABLE \n",
            "THE BOOK IS ON TE TABLE \n",
            "THE BOOK IS ONTHE TABLE \n",
            "THE BOOKIS ON THE TABLE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZYziaWGfQNI"
      },
      "source": [
        "# Load Audio and transcribe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbLohZCsj_Lr",
        "outputId": "e71c4dc7-17bd-461f-9285-db7edeed5169"
      },
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/c/c8/Example.ogg -O example.ogg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 10:46:51--  https://upload.wikimedia.org/wikipedia/commons/c/c8/Example.ogg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 91.198.174.208, 2620:0:862:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|91.198.174.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 105243 (103K) [application/ogg]\n",
            "Saving to: ‘example.ogg’\n",
            "\n",
            "\rexample.ogg           0%[                    ]       0  --.-KB/s               \rexample.ogg         100%[===================>] 102.78K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-04-29 10:46:51 (6.88 MB/s) - ‘example.ogg’ saved [105243/105243]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n02-PCqmR_gu"
      },
      "source": [
        "### Audio Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg_1fjOtjw_S"
      },
      "source": [
        "from scipy.signal import resample\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "class AudioReader:\n",
        "    def __init__(self, audio_path, sr=16000, dtype=\"float32\"):\n",
        "        self._sr = sr\n",
        "        self._dtype = dtype\n",
        "        self._audio_path = audio_path\n",
        "    \n",
        "    def read(self):\n",
        "        data, sr = sf.read(self._audio_path, dtype=self._dtype)\n",
        "        data = self.__resample_file(data, sr, self._sr)\n",
        "        return data, self._sr\n",
        "\n",
        "    def __resample_file(self, array, original_sr, target_sr):\n",
        "        return resample(array, num=int(len(array)*target_sr/original_sr))\n",
        "\n",
        "class AudioStreaming:\n",
        "    def __init__(self, audio_path, blocksize, sr=16000, overlap=0, padding=None, dtype=\"float32\"):\n",
        "        assert blocksize >= 0, \"blocksize cannot be 0 or negative\"\n",
        "        self._sr = sr\n",
        "        self._orig_sr = sf.info(audio_path).samplerate\n",
        "        self._sf_blocks = sf.blocks(audio_path,\n",
        "                        blocksize=blocksize, \n",
        "                        overlap=overlap,\n",
        "                        fill_value=padding,\n",
        "                        dtype=dtype)\n",
        "\n",
        "    def generator(self):\n",
        "        for block in self._sf_blocks:\n",
        "            chunk = self.__resample_file(block, self._orig_sr, self._sr)\n",
        "            yield chunk, self._orig_sr\n",
        "\n",
        "    def __resample_file(self, array, original_sr, target_sr):\n",
        "        return resample(array, num=int(len(array)*target_sr/original_sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIkuak68SGHc"
      },
      "source": [
        "### Loading Audio one pass\n",
        "\n",
        "load full audio at once and transcribe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INZ5mzeR5ky8"
      },
      "source": [
        "audio_reader = AudioReader(\"/content/example.ogg\", sr=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4MQqAwq56H-",
        "outputId": "35bafb39-f4c0-4d82-a89e-d732fc51f91e"
      },
      "source": [
        "block, sr = audio_reader.read()\n",
        "print(sr)\n",
        "print(block.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000\n",
            "(97664, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAo_nEkL5KIk"
      },
      "source": [
        "input_values = tokenizer(block[:,0], return_tensors = 'pt').input_values\n",
        "#logits (non-normalized predictions)\n",
        "logits = model(input_values).logits\n",
        "out_proba = torch.nn.functional.softmax(logits, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR5C7RFp5Ovw"
      },
      "source": [
        "predicted_ids = torch.argmax(out_proba, dim =-1)\n",
        "results_ = tokenizer.decode(predicted_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmDCqLE35RU-",
        "outputId": "baceac94-789e-4521-bab3-0775218f31b9"
      },
      "source": [
        "print(\"Without Language Model\")\n",
        "print(results_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Language Model\n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUGFORBUS FORMAT FROM WICKIPAEDIA THE FREE INCYCLOPAEDIA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcZrXDNs5RSn"
      },
      "source": [
        "results = decode_and_rescore(out_proba, num_sentences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L7Y1IgM5RPf",
        "outputId": "3fae975a-91cc-48fc-da65-260c152b50da"
      },
      "source": [
        "print(\"With Language Model Kenlm\")\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With Language Model Kenlm\n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUG FORBUS FORMAT FROM WICKIPAEDIA THE FREE IN CYCLOPAEDIA \n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUG FORBUS FORMAT FROM WICKIPAEDIA THE FREE INCYCLOPAEDIA \n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUG FORBUS FORMAT FROM WICKI PAEDIA THE FREE IN CYCLOPAEDIA \n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUG FORBUS FORMAT FROM WICKI PAEDIA THE FREE INCYCLOPAEDIA \n",
            "THIS IS AN EXAMPLE SOUND FILE IN AUGFORBUS FORMAT FROM WICKIPAEDIA THE FREE IN CYCLOPAEDIA \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8fHckkcSnFC"
      },
      "source": [
        "### Splitting audio and load\n",
        "\n",
        "first split audio into multiple blocks and pass each block for transcribing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llt7vvH6iI3X"
      },
      "source": [
        "## splitting 5 sec\n",
        "audio_stream = AudioStreaming(audio_path=\"/content/example.ogg\", blocksize=16000*5, padding=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amMd8MtFPvwr",
        "outputId": "4d7eedbc-6b21-49b7-a3c5-560062cf71bb"
      },
      "source": [
        "for block, sr in audio_stream.generator():\n",
        "  inputs = tokenizer(block[:,0], return_tensors='pt').input_values\n",
        "  logits = model(inputs).logits\n",
        "  predicted_ids = torch.argmax(logits, dim =-1)\n",
        "  print(tokenizer.decode(predicted_ids[0]), end=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBUS FORMANWIKIPAEDIA THE FREE INCYPATAVIA"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6f6ipKmS_AX"
      },
      "source": [
        "### transcribe block and decode a last"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf3kBm77TKQQ"
      },
      "source": [
        "## splitting 5 sec\n",
        "audio_stream = AudioStreaming(audio_path=\"/content/example.ogg\", blocksize=16000*5, padding=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UiCIOo5wjsQ"
      },
      "source": [
        "ctc_outs = torch.Tensor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxifzA1wkLE7"
      },
      "source": [
        "for block, sr in audio_stream.generator():\n",
        "  inputs = tokenizer(block[:,0], return_tensors='pt').input_values\n",
        "  logits = model(inputs).logits\n",
        "  logits = torch.nn.functional.softmax(logits, dim=-1)\n",
        "  ctc_outs = torch.cat((ctc_outs, logits), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE1NAF9A4L0x"
      },
      "source": [
        "results = decode_and_rescore(ctc_outs, num_sentences=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiQQf6Vc4osZ",
        "outputId": "5da974c5-28fa-4980-c9f7-8e488285b8c8"
      },
      "source": [
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBUS FORMAN WICIPAEDIA THE FREE INCYPATAVIA \n",
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBIS FORMAN WICIPAEDIA THE FREE INCYPATAVIA \n",
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBUS FORMAN WICIPAEDIA THE FREE IN CYPATAVIA \n",
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBUS FORMAN WICKIPAEDIA THE FREE INCYPATAVIA \n",
            "THIS IS AN EXAMPLE SOUND FILIN AUG FORBUS FORMAN WIKIPAEDIA THE FREE INCYPATAVIA \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8f2WZe3ePyQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}